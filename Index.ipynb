{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project title: Sentiment Analysis on Tweets about Brands and Products\n",
    "\n",
    "\n",
    "- Student names: \n",
    "    - INGAVI KILAVUKA\n",
    "    - CALVIN OMWEGA\n",
    "    - ALVIN KIMATHI\n",
    "    - Ronny Kabiru\n",
    "- Instructor name: Maryann Mwikali\n",
    "- DATASET/MODELING FOCUS: NLP(Natural language processing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "To analyze the sentiment of tweets directed at various brands and products to understand public perception. This will involve determining whether the sentiment expressed in tweets is positive, negative, or neutral and identifying the specific brands and products mentioned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Definition:\n",
    "In the age of social media, understanding public sentiment towards brands and products is crucial for businesses. This project aims to:\n",
    "1. **Identify the Sentiment**: Determine whether the sentiment expressed in each tweet is positive, negative, or neutral.\n",
    "2. **Target Identification**: Identify the specific brand or product that the sentiment is directed towards.\n",
    "3. **Trends and Insights**: Derive insights and trends regarding public perception of various brands and products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Questions:\n",
    "1. **Sentiment Classification**: What is the distribution of positive, negative, and neutral sentiments across the dataset?\n",
    "2. **Brand/Product Association**: Which brands and products are most frequently mentioned, and what is the associated sentiment for each?\n",
    "3. **Temporal Trends**: Are there any notable trends in sentiment over time for specific brands or products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.22.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (20.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (50.3.0.post20201103)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.32.0)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.1.1)\n",
      "\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (8.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikerasNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached scikeras-0.12.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: packaging>=0.21 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from scikeras) (20.4)\n",
      "Collecting scikit-learn>=1.0.0 (from scikeras)\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem<0.32,>=0.23.1 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from scikeras) (0.31.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging>=0.21->scikeras) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging>=0.21->scikeras) (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.22.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.5.0)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn>=1.0.0->scikeras)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (2.1.0)\n",
      "Using cached scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 9.3/9.3 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 301.8/301.8 kB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: joblib, scikit-learn, scikeras\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 0.17.0\n",
      "    Uninstalling joblib-0.17.0:\n",
      "      Successfully uninstalled joblib-0.17.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\n",
      "\n",
      "    Uninstalling scikit-learn-0.23.2:\n",
      "      Successfully uninstalled scikit-learn-0.23.2\n",
      "Successfully installed joblib-1.4.2 scikeras-0.12.0 scikit-learn-1.3.2\n"
     ]
    }
   ],
   "source": [
    "pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from nltk>=3.8->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from nltk>=3.8->textblob) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\homepc\\anaconda3\\envs\\learn-env\\lib\\site-packages (from nltk>=3.8->textblob) (4.50.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'defun_with_attributes' from 'tensorflow.python.eager.function' (c:\\Users\\HomePC\\anaconda3\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1cba35a129e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HomePC\\anaconda3\\envs\\learn-env\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HomePC\\anaconda3\\envs\\learn-env\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmixed_precision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HomePC\\anaconda3\\envs\\learn-env\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\function\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdef_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefun_with_attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'defun_with_attributes' from 'tensorflow.python.eager.function' (c:\\Users\\HomePC\\anaconda3\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py)"
     ]
    }
   ],
   "source": [
    "#libraries \n",
    "# import necessary libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, GRU\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('judge_1377884607_tweet_product_company.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8721, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:\n",
      "['tweet_text', 'emotion_in_tweet_is_directed_at', 'is_there_an_emotion_directed_at_a_brand_or_product']\n",
      "\n",
      "\n",
      "Data Types:\n",
      "tweet_text                                            object\n",
      "emotion_in_tweet_is_directed_at                       object\n",
      "is_there_an_emotion_directed_at_a_brand_or_product    object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Shape of the DataFrame:\n",
      "Rows: 8721, Columns: 3\n",
      "\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8721 entries, 0 to 8720\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          8720 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3169 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  8721 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 204.5+ KB\n",
      "\n",
      "\n",
      "Descriptive Statistics:\n",
      "                                               tweet_text  \\\n",
      "count                                                8720   \n",
      "unique                                               8693   \n",
      "top     RT @mention Marissa Mayer: Google Will Connect...   \n",
      "freq                                                    5   \n",
      "\n",
      "       emotion_in_tweet_is_directed_at  \\\n",
      "count                             3169   \n",
      "unique                               9   \n",
      "top                               iPad   \n",
      "freq                               910   \n",
      "\n",
      "       is_there_an_emotion_directed_at_a_brand_or_product  \n",
      "count                                                8721  \n",
      "unique                                                  4  \n",
      "top                    No emotion toward brand or product  \n",
      "freq                                                 5156  \n",
      "\n",
      "\n",
      "Missing Values (Percentage):\n",
      "tweet_text                                             0.011467\n",
      "emotion_in_tweet_is_directed_at                       63.662424\n",
      "is_there_an_emotion_directed_at_a_brand_or_product     0.000000\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Categorical Data Summary:\n",
      "                                               tweet_text  \\\n",
      "count                                                8720   \n",
      "unique                                               8693   \n",
      "top     RT @mention Marissa Mayer: Google Will Connect...   \n",
      "freq                                                    5   \n",
      "\n",
      "       emotion_in_tweet_is_directed_at  \\\n",
      "count                             3169   \n",
      "unique                               9   \n",
      "top                               iPad   \n",
      "freq                               910   \n",
      "\n",
      "       is_there_an_emotion_directed_at_a_brand_or_product  \n",
      "count                                                8721  \n",
      "unique                                                  4  \n",
      "top                    No emotion toward brand or product  \n",
      "freq                                                 5156  \n",
      "\n",
      "\n",
      "Value Counts for Categorical Columns:\n",
      "Column: tweet_text\n",
      "RT @mention Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #sxsw                                     5\n",
      "RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw                                                  4\n",
      "RT @mention Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #SXSW                                     4\n",
      "RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #SXSW                                                  3\n",
      "Win free ipad 2 from webdoc.com #sxsw RT                                                                                                           2\n",
      "                                                                                                                                                  ..\n",
      "RT @mention RT @mention Beautiful day in #austin! Come by and grab a slice or two. If you check in on #google, you can get a deal. #sxswi #SXSW    1\n",
      "Toddlers on iPad *get* direct interaction of content. Don't get trapped in metaphors &amp; hacks. @mention #tapworthy #sxsw                        1\n",
      "Arrived in Austin for #SXSW. Our hotel room is B.Y.O.A.P. (bring your own Apple products). My MBP, wife's iMac, iPad.                              1\n",
      "���@mention Spotted at #SXSW: Man in need of an iPad (2?) ��� {link} #sxswi�۝ man in need should go to SXSW apple store                            1\n",
      "NaN                                                                                                                                                1\n",
      "Name: tweet_text, Length: 8694, dtype: int64\n",
      "\n",
      "\n",
      "Column: emotion_in_tweet_is_directed_at\n",
      "NaN                                5552\n",
      "iPad                                910\n",
      "Apple                               640\n",
      "iPad or iPhone App                  451\n",
      "Google                              412\n",
      "iPhone                              288\n",
      "Other Google product or service     282\n",
      "Android App                          78\n",
      "Android                              74\n",
      "Other Apple product or service       34\n",
      "Name: emotion_in_tweet_is_directed_at, dtype: int64\n",
      "\n",
      "\n",
      "Column: is_there_an_emotion_directed_at_a_brand_or_product\n",
      "No emotion toward brand or product    5156\n",
      "Positive emotion                      2869\n",
      "Negative emotion                       545\n",
      "I can't tell                           151\n",
      "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_dataset(df):\n",
    "    # Display column names\n",
    "    print(\"Column Names:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Display data types\n",
    "    print(\"Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Display number of rows and columns\n",
    "    print(\"Shape of the DataFrame:\")\n",
    "    print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # DataFrame info\n",
    "    print(\"DataFrame Info:\")\n",
    "    df.info()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Descriptive statistics for numerical columns\n",
    "    print(\"Descriptive Statistics:\")\n",
    "    print(df.describe())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Missing values summary\n",
    "    print(\"Missing Values (Percentage):\")\n",
    "    missing_values_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    print(missing_values_percentage)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Display summary statistics for categorical columns\n",
    "    print(\"Categorical Data Summary:\")\n",
    "    categorical_summary = df.select_dtypes(include=['object']).describe()\n",
    "    print(categorical_summary)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Value counts for categorical columns\n",
    "    print(\"Value Counts for Categorical Columns:\")\n",
    "    for column in df.select_dtypes(include=['object']).columns:\n",
    "        print(f\"Column: {column}\")\n",
    "        print(df[column].value_counts(dropna=False))\n",
    "        print(\"\\n\")\n",
    "\n",
    "explore_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['tweet_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...  \n",
       "3  @sxsw I hope this year's festival isn't as cra...  \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the function to the tweet column\n",
    "df['cleaned_text'] = df['tweet_text'].apply(remove_urls)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "df[['tweet_text', 'cleaned_text']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>wesley83 I have a 3G iPhone After 3 hrs tweeti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>jessedee Know about fludapp  Awesome iPadiPhon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>swonderlin Can not wait for iPad 2 also They s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>sxsw I hope this years festival isnt as crashy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>sxtxstate great stuff on Fri SXSW Marissa Maye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  wesley83 I have a 3G iPhone After 3 hrs tweeti...  \n",
       "1  jessedee Know about fludapp  Awesome iPadiPhon...  \n",
       "2  swonderlin Can not wait for iPad 2 also They s...  \n",
       "3  sxsw I hope this years festival isnt as crashy...  \n",
       "4  sxtxstate great stuff on Fri SXSW Marissa Maye...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Apply the function to the cleaned_text column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_special_characters)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "df[['tweet_text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HomePC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>wesley83 3G iPhone 3 hrs tweeting RISEAustin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>jessedee Know fludapp Awesome iPadiPhone app y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>swonderlin wait iPad 2 also sale SXSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>sxsw hope years festival isnt crashy years iPh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>sxtxstate great stuff Fri SXSW Marissa Mayer G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  wesley83 3G iPhone 3 hrs tweeting RISEAustin d...  \n",
       "1  jessedee Know fludapp Awesome iPadiPhone app y...  \n",
       "2              swonderlin wait iPad 2 also sale SXSW  \n",
       "3  sxsw hope years festival isnt crashy years iPh...  \n",
       "4  sxtxstate great stuff Fri SXSW Marissa Mayer G...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# Apply the function to the cleaned_text column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "df[['tweet_text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HomePC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>wesley83 3G iPhone 3 hrs tweeting RISEAustin d...</td>\n",
       "      <td>[wesley83, 3G, iPhone, 3, hrs, tweeting, RISEA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>jessedee Know fludapp Awesome iPadiPhone app y...</td>\n",
       "      <td>[jessedee, Know, fludapp, Awesome, iPadiPhone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>swonderlin wait iPad 2 also sale SXSW</td>\n",
       "      <td>[swonderlin, wait, iPad, 2, also, sale, SXSW]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>sxsw hope years festival isnt crashy years iPh...</td>\n",
       "      <td>[sxsw, hope, years, festival, isnt, crashy, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>sxtxstate great stuff Fri SXSW Marissa Mayer G...</td>\n",
       "      <td>[sxtxstate, great, stuff, Fri, SXSW, Marissa, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  wesley83 3G iPhone 3 hrs tweeting RISEAustin d...   \n",
       "1  jessedee Know fludapp Awesome iPadiPhone app y...   \n",
       "2              swonderlin wait iPad 2 also sale SXSW   \n",
       "3  sxsw hope years festival isnt crashy years iPh...   \n",
       "4  sxtxstate great stuff Fri SXSW Marissa Mayer G...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [wesley83, 3G, iPhone, 3, hrs, tweeting, RISEA...  \n",
       "1  [jessedee, Know, fludapp, Awesome, iPadiPhone,...  \n",
       "2      [swonderlin, wait, iPad, 2, also, sale, SXSW]  \n",
       "3  [sxsw, hope, years, festival, isnt, crashy, ye...  \n",
       "4  [sxtxstate, great, stuff, Fri, SXSW, Marissa, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download punkt for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply the function to the cleaned_text column\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "df[['tweet_text', 'cleaned_text', 'tokens']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Positive': 4299, 'Neutral': 3229, 'Negative': 1192})\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "\n",
    "# Function to perform sentiment analysis and create a frequency table\n",
    "def analyze_sentiments_and_plot_frequency(df, tweet_text):\n",
    "    # Creating a TextBlob object for each tweet and calculate sentiment\n",
    "    df['sentiment'] = df[tweet_text].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "    # Define sentiment labels\n",
    "    def get_sentiment_label(sentiment):\n",
    "        if sentiment > 0:\n",
    "            return 'Positive'\n",
    "        elif sentiment < 0:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    df['sentiment_label'] = df['sentiment'].apply(get_sentiment_label)\n",
    "    \n",
    "    # Create a frequency table for sentiment labels\n",
    "    sentiment_counts = Counter(df['sentiment_label'])\n",
    "    return sentiment_counts\n",
    "\n",
    "sentiment_counts = analyze_sentiments_and_plot_frequency(df, 'tweet_text')\n",
    "analyze_sentiments_and_plot_frequency(df, 'tweet_text')\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define colors for the bar plot\n",
    "colors = {\n",
    "    'Positive': '#9b7179',\n",
    "    'Negative': '#cda2aa',\n",
    "    'Neutral': '#e5b5be'\n",
    "}\n",
    "\n",
    "# Plot a bar graph for sentiment frequency with a legend\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(sentiment_counts.keys(), sentiment_counts.values(), color=[colors[label] for label in sentiment_counts.keys()])\n",
    "\n",
    "# Add legend\n",
    "for bar, label in zip(bars, sentiment_counts.keys()):\n",
    "    bar.set_label(label)\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
